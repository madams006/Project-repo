{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinkedIn Job Scraper - Complete with Natural Language Processing utilization for Resume & Job Description Comparisons\n",
    "\n",
    "Today we will be looking at an implementation of the Selenium library to first scrape LinkedIn Jobs postings by Title and Location, and then return Title, Location, Description, as well as the Company who posted the Job. To do so, all you need to do is input your credentials below, as well as what Job Title you are looking for, where you are looking, and how many pages worth of scraping you would like to do on LinkedIn.\n",
    "\n",
    "Once these jobs are scraped, we will then call on Natural Language Processing in combination with sklearn's CountVectorizer and cosine_similarity to determine how efficient our resume may be in applying for the job.\n",
    "\n",
    "This project can be further adapted to perform analysis on the contents of the resume, as well as create visualizations, such as word clouds, to show the most common skills required of a given job; i.e. a Data Scientist likely will mention analysis, Python, and Data quite often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "\n",
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = []\n",
    "companies = []\n",
    "locations = []\n",
    "job_descs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Data Scientist\"]\n",
    "location = [\"New York\"]\n",
    "email = \"madamsgolf239@gmail.com\"\n",
    "password = \"@Riannascanlon1\"\n",
    "resume=r\"C:\\Users\\musta\\OneDrive\\Resumes NEW\\Michael_Adams_Resume_2022.pdf\" #CHANGE THIS TO YOUR RESUME'S PATH - NOTE, this takes both PDF & Word Files.\n",
    "pages = 2#enter an integer (Try 2 at first, due to loading of web pages it will take a couple of minutes to run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LI_Scraper(keywords, location, email, password, pages):\n",
    "    # direct the webdriver to where the browser file is:\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    # your secret credentials:\n",
    "    driver.maximize_window()\n",
    "    # Go to linkedin and login\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.ID,'username').send_keys(email)\n",
    "    driver.find_element(By.ID,'password').send_keys(password)\n",
    "    driver.find_element(By.ID,'password').send_keys(Keys.RETURN)\n",
    "\n",
    "    driver.get(\"https://www.linkedin.com/jobs/\")\n",
    "    time.sleep(3)\n",
    "        # find the keywords/location search bars:\n",
    "    search_bars = driver.find_elements(By.CLASS_NAME,'jobs-search-box__text-input')\n",
    "    search_keywords = search_bars[0]\n",
    "        #time.sleep(3)\n",
    "    search_bars[0].click()\n",
    "    time.sleep(3)\n",
    "    search_bars[0].send_keys(keywords)\n",
    "    time.sleep(3)\n",
    "    search_bars[0].send_keys(Keys.TAB)\n",
    "    time.sleep(3)\n",
    "    search_bars[3].send_keys(location)\n",
    "    time.sleep(3)\n",
    "    search_bars[3].send_keys(Keys.RETURN)\n",
    "    time.sleep(5)\n",
    "    #earch_url = driver.current_url\n",
    "    #job = driver.find_elements(By.CLASS_NAME, \"job-card-list__title\")\n",
    "    #job_titles = []\n",
    "   # companies = []\n",
    "    #locations = []\n",
    "    #job_descs = []\n",
    "\n",
    "    for page in range(1,int(pages)):\n",
    "        time.sleep(3)\n",
    "        page = 1\n",
    "        if page > 1:\n",
    "            driver.find_element(By.XPATH,f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/section/div/ul/li[{page}]/button').click()\n",
    "            time.sleep(3)\n",
    "            job2 = driver.find_elements(By.CLASS_NAME, \"job-card-list__title\")\n",
    "            for j in job2:\n",
    "                print(j.text)\n",
    "                job_titles.append(j.text)\n",
    "            #print(c)\n",
    "\n",
    "            company2 = driver.find_elements(By.CLASS_NAME, \"job-card-container__company-name\")\n",
    "            for c in company2:\n",
    "                print(c.text)\n",
    "                companies.append(c.text)\n",
    "\n",
    "\n",
    "            location2 = driver.find_elements(By.CLASS_NAME, \"job-card-container__metadata-item\")\n",
    "            for l in location2:\n",
    "                print(l.text)\n",
    "                locations.append(l.text)\n",
    "\n",
    "            for j in range(1, len(job)+1):\n",
    "                # job click\n",
    "                time.sleep(1)\n",
    "                element = driver.find_element(By.XPATH,f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{j}]/div/div/div[1]/div[2]/div[1]/a')\n",
    "                # driver.execute_script(\"arguments[0].scrollIntoView()\", element)\n",
    "                time.sleep(1)\n",
    "                element.click()\n",
    "                # waiting load\n",
    "                time.sleep(2)\n",
    "                job_description = driver.find_element(By.CLASS_NAME, f'jobs-description__container').text\n",
    "                #job_description = job_description.replace('\\n','')\n",
    "                job_descs.append(job_description)\n",
    "                time.sleep(5)\n",
    "                page += 1\n",
    "        else:\n",
    "            time.sleep(3)\n",
    "            #search_url = driver.current_url\n",
    "            job = driver.find_elements(By.CLASS_NAME, \"job-card-list__title\")\n",
    "            for j in job:\n",
    "                print(j.text)\n",
    "                job_titles.append(j.text)\n",
    "            #print(c)\n",
    "\n",
    "            company = driver.find_elements(By.CLASS_NAME, \"job-card-container__company-name\")\n",
    "            for c in company:\n",
    "                print(c.text)\n",
    "                companies.append(c.text)\n",
    "\n",
    "\n",
    "            location = driver.find_elements(By.CLASS_NAME, \"job-card-container__metadata-item\")\n",
    "            for l in location:\n",
    "                print(l.text)\n",
    "                locations.append(l.text)\n",
    "\n",
    "            for j in range(1, len(job)+1):\n",
    "                # job click\n",
    "                time.sleep(1)\n",
    "                element = driver.find_element(By.XPATH,f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{j}]/div/div/div[1]/div[2]/div[1]/a')\n",
    "                # driver.execute_script(\"arguments[0].scrollIntoView()\", element)\n",
    "                time.sleep(1)\n",
    "                element.click()\n",
    "                # waiting load\n",
    "                time.sleep(2)\n",
    "                job_description = driver.find_element(By.CLASS_NAME, f'jobs-description__container').text\n",
    "                #job_description = job_description.replace('\\n','')\n",
    "                job_descs.append(job_description)\n",
    "                time.sleep(5)\n",
    "        #return(job_descs)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_resume(file):\n",
    "    '''Opens & reads in a PDF file from path'''\n",
    "    \n",
    "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "    page_count = fileReader.getNumPages()\n",
    "    text = [fileReader.getPage(i).extractText() for i in range(page_count)]\n",
    "    \n",
    "    return str(text).replace(\"\\\\n\", \"\")\n",
    "\n",
    "def read_word_resume(filepath):\n",
    "    '''Opens & reads in a .doc or .docx file from path'''\n",
    "    \n",
    "    txt = textract.process(filepath).decode('utf-8')\n",
    "    \n",
    "    return txt.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "def read_resume(infile):\n",
    "    '''Takes an infile and attempts to read it as docx file,\n",
    "    if it is pdf will read as pdf as read_word_resume will not work'''\n",
    "    try:\n",
    "        resume =  read_word_resume(infile)\n",
    "    except:\n",
    "        resume = read_pdf_resume(infile)\n",
    "    text_resume = str(resume)\n",
    "    return(text_resume)\n",
    "\n",
    "def get_resume_score(text):\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    \n",
    "    count_matrix = cv.fit_transform(text)\n",
    "    #Print the similarity scores\n",
    "    #print(\"\\nSimilarity Scores:\")\n",
    "     \n",
    "    #get the match percentage\n",
    "    matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
    "    matchPercentage = round(matchPercentage, 2) # round to two decimal\n",
    "     \n",
    "    #print(\"Your resume matches about \"+ str(matchPercentage)+ \"% of the job description.\")\n",
    "    score_list.append(matchPercentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "There is no [win32] chromedriver for browser 100.0.4896 in cache\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/100.0.4896.60/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\musta\\.wdm\\drivers\\chromedriver\\win32\\100.0.4896.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment Data Scientist\n",
      "Data Scientist, Basketball Integrity\n",
      "Data Scientist\n",
      "Data Scientist, Product Analytics - Business Integrity\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "DURLSTON PARTNERS\n",
      "National Basketball Association (NBA)\n",
      "Altus Group\n",
      "Meta\n",
      "GAIUS Networks (Flipped.ai)\n",
      "XL-Data Corp\n",
      "Whiterock.ai\n",
      "New York, United States\n",
      "On-site\n",
      "New York County, NY\n",
      "On-site\n",
      "New York, NY\n",
      "On-site\n",
      "New York, NY\n",
      "United States\n",
      "Remote\n",
      "United States\n",
      "Remote\n",
      "New York, NY\n",
      "Hybrid\n"
     ]
    }
   ],
   "source": [
    "LI_Scraper(keywords, location, email, password, pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "def see_results():\n",
    "    for job_desc in job_descs:\n",
    "        #print(job_desc)\n",
    "        resume_text = read_resume(resume)\n",
    "        for job_desc in job_descs:\n",
    "            text = [resume_text, job_desc]\n",
    "            get_resume_score(text)\n",
    "            \n",
    "        for score, company, job_title in zip(score_list, companies, job_titles):\n",
    "            print(f\"Your resume matches approximately {score}% of the {job_title} position at {company}.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your resume matches approximately 10.65% of the Investment Data Scientist position at DURLSTON PARTNERS.\n",
      "Your resume matches approximately 20.09% of the Data Scientist, Basketball Integrity position at National Basketball Association (NBA).\n",
      "Your resume matches approximately 21.85% of the Data Scientist position at Altus Group.\n",
      "Your resume matches approximately 12.21% of the Data Scientist, Product Analytics - Business Integrity position at Meta.\n",
      "Your resume matches approximately 23.23% of the Data Scientist position at GAIUS Networks (Flipped.ai).\n",
      "Your resume matches approximately 16.01% of the Data Scientist position at XL-Data Corp.\n",
      "Your resume matches approximately 15.71% of the Data Scientist position at Whiterock.ai.\n",
      "Your resume matches approximately 10.65% of the Investment Data Scientist position at DURLSTON PARTNERS.\n",
      "Your resume matches approximately 20.09% of the Data Scientist, Basketball Integrity position at National Basketball Association (NBA).\n",
      "Your resume matches approximately 21.85% of the Data Scientist position at Altus Group.\n",
      "Your resume matches approximately 12.21% of the Data Scientist, Product Analytics - Business Integrity position at Meta.\n",
      "Your resume matches approximately 23.23% of the Data Scientist position at GAIUS Networks (Flipped.ai).\n",
      "Your resume matches approximately 16.01% of the Data Scientist position at XL-Data Corp.\n",
      "Your resume matches approximately 15.71% of the Data Scientist position at Whiterock.ai.\n",
      "Your resume matches approximately 10.65% of the Investment Data Scientist position at DURLSTON PARTNERS.\n",
      "Your resume matches approximately 20.09% of the Data Scientist, Basketball Integrity position at National Basketball Association (NBA).\n",
      "Your resume matches approximately 21.85% of the Data Scientist position at Altus Group.\n",
      "Your resume matches approximately 12.21% of the Data Scientist, Product Analytics - Business Integrity position at Meta.\n",
      "Your resume matches approximately 23.23% of the Data Scientist position at GAIUS Networks (Flipped.ai).\n",
      "Your resume matches approximately 16.01% of the Data Scientist position at XL-Data Corp.\n",
      "Your resume matches approximately 15.71% of the Data Scientist position at Whiterock.ai.\n",
      "Your resume matches approximately 10.65% of the Investment Data Scientist position at DURLSTON PARTNERS.\n",
      "Your resume matches approximately 20.09% of the Data Scientist, Basketball Integrity position at National Basketball Association (NBA).\n",
      "Your resume matches approximately 21.85% of the Data Scientist position at Altus Group.\n",
      "Your resume matches approximately 12.21% of the Data Scientist, Product Analytics - Business Integrity position at Meta.\n",
      "Your resume matches approximately 23.23% of the Data Scientist position at GAIUS Networks (Flipped.ai).\n",
      "Your resume matches approximately 16.01% of the Data Scientist position at XL-Data Corp.\n",
      "Your resume matches approximately 15.71% of the Data Scientist position at Whiterock.ai.\n",
      "Your resume matches approximately 10.65% of the Investment Data Scientist position at DURLSTON PARTNERS.\n",
      "Your resume matches approximately 20.09% of the Data Scientist, Basketball Integrity position at National Basketball Association (NBA).\n",
      "Your resume matches approximately 21.85% of the Data Scientist position at Altus Group.\n",
      "Your resume matches approximately 12.21% of the Data Scientist, Product Analytics - Business Integrity position at Meta.\n",
      "Your resume matches approximately 23.23% of the Data Scientist position at GAIUS Networks (Flipped.ai).\n",
      "Your resume matches approximately 16.01% of the Data Scientist position at XL-Data Corp.\n",
      "Your resume matches approximately 15.71% of the Data Scientist position at Whiterock.ai.\n",
      "Your resume matches approximately 10.65% of the Investment Data Scientist position at DURLSTON PARTNERS.\n",
      "Your resume matches approximately 20.09% of the Data Scientist, Basketball Integrity position at National Basketball Association (NBA).\n",
      "Your resume matches approximately 21.85% of the Data Scientist position at Altus Group.\n",
      "Your resume matches approximately 12.21% of the Data Scientist, Product Analytics - Business Integrity position at Meta.\n",
      "Your resume matches approximately 23.23% of the Data Scientist position at GAIUS Networks (Flipped.ai).\n",
      "Your resume matches approximately 16.01% of the Data Scientist position at XL-Data Corp.\n",
      "Your resume matches approximately 15.71% of the Data Scientist position at Whiterock.ai.\n",
      "Your resume matches approximately 10.65% of the Investment Data Scientist position at DURLSTON PARTNERS.\n",
      "Your resume matches approximately 20.09% of the Data Scientist, Basketball Integrity position at National Basketball Association (NBA).\n",
      "Your resume matches approximately 21.85% of the Data Scientist position at Altus Group.\n",
      "Your resume matches approximately 12.21% of the Data Scientist, Product Analytics - Business Integrity position at Meta.\n",
      "Your resume matches approximately 23.23% of the Data Scientist position at GAIUS Networks (Flipped.ai).\n",
      "Your resume matches approximately 16.01% of the Data Scientist position at XL-Data Corp.\n",
      "Your resume matches approximately 15.71% of the Data Scientist position at Whiterock.ai.\n"
     ]
    }
   ],
   "source": [
    "see_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Investment Data Scientist',\n",
       " 'Data Scientist, Basketball Integrity',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist, Product Analytics - Business Integrity',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\musta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docx2txt\n",
    "import nltk\n",
    "import requests\n",
    "\n",
    "API_KEY = \"J1rKQq3o2BsDdiC5jRoURWLb485qJ96I\"\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skill_exists(skill):\n",
    "    url = f'https://api.apilayer.com/skills?q={skill}&amp;count=1'\n",
    "    headers = {'apikey': str(API_KEY)}\n",
    "    response = requests.request('GET', url, headers=headers)\n",
    "    result = response.json()\n",
    " \n",
    "    if response.status_code == 200:\n",
    "        return len(result) > 0 and result[0].lower() == skill.lower()\n",
    "    raise Exception(result.get('message'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_database = ['machine learning', 'deep learning', 'computer vision', 'data science', 'data analysis',\n",
    "                    'predictive analytics', 'business intelligence', 'artificial intelligence','ai','python','SQL',\n",
    "                    'Excel', 'Database Management']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    " \n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    " \n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    " \n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    " \n",
    "     # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in skills_database:\n",
    "            found_skills.add(token)\n",
    " \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in skills_database:\n",
    "            found_skills.add(ngram)\n",
    " \n",
    "    return found_skills\n",
    "    #UNCOMMENT BELOW TO USE TOKENS TO SCRAPE API\n",
    "    # # we search for each token in our skills database\n",
    "    # for token in filtered_tokens:\n",
    "    #     if skill_exists(token.lower()):\n",
    "    #         found_skills.add(token)\n",
    " \n",
    "    # # we search for each bigram and trigram in our skills database\n",
    "    # for ngram in bigrams_trigrams:\n",
    "    #     if skill_exists(ngram.lower()):\n",
    "    #         found_skills.add(ngram)\n",
    " \n",
    "    # return found_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These skills are found on both your resume and in our skills database{'Computer Vision', 'Data Science', 'AI', 'Deep Learning', 'deep learning', 'Data Analysis', 'Python'}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    text = read_resume(r\"C:\\Users\\musta\\OneDrive\\Resumes NEW\\Michael_Adams_Resume_2022.pdf\")\n",
    "    extracted_skills = extract_skills(text)\n",
    "    print(f\"These skills are found on both your resume and in our skills database{extracted_skills}\")\n",
    "    return extracted_skills\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These skills are found on both your resume and in our skills database{'Computer Vision', 'Data Science', 'AI', 'Deep Learning', 'deep learning', 'Data Analysis', 'Python'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-7878e78b0ba2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mskills\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Specify differnt figures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskills\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "skills = main()\n",
    "plt.figure(0)  # Specify differnt figures\n",
    "plt.barh(skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d1de90774c993cf6e7d5c386b4ee54de413cea9aa0ff5448672a6f040e044d5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
